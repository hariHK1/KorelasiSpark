# -*- coding: utf-8 -*-
"""upload.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sNiFtagKQaeoNwg5ti6-1xLdgi9ilqGu
"""

# Takes some time 3-5 minutes
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz #sesuaikan dengan hadoop yang dipakai
!tar xf spark-3.4.1-bin-hadoop3.tgz
!pip install -q findspark

import os
import findspark
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"
findspark.init()

!pip install hdfs
import hdfs

from hdfs import InsecureClient

# Create an HDFS client
client = InsecureClient('http://bigdata.bisa.ai:9870', user='root')

# Specify the local file path and the HDFS destination path
hdfs_destination_path = '/diabetes.csv'  # Adjust the HDFS path as needed
local_file_path = '/content/diabetes.csv'

# Upload the file to HDFS
client.download(hdfs_destination_path, local_file_path)

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("PosturedanPersonality_1").getOrCreate()

df = spark.read.csv("PosturedanPersonality_1.csv", header=True, inferSchema=True)

df.show()

df.printSchema()

df.describe().toPandas()

df.groupby('POSTURE').count().show()

import matplotlib.pyplot as plt
import seaborn as sns

fig = plt.figure(figsize=(25, 15))
st = fig.suptitle("Distribution of Features", fontsize=50, verticalalignment="center")
for col, num in zip(df.toPandas().describe().columns, range(1,11)):
  ax = fig.add_subplot(3,4, num)
  ax.hist(df.toPandas()[col])
  plt.grid(False)
  plt.xticks(rotation=45, fontsize=20)
  plt.yticks(fontsize=15)
  plt.title(col.upper(), fontsize=20)

plt.tight_layout()
st.set_y(0.95)
fig.subplots_adjust(top=0.85, hspace=0.4)
plt.show()

from pyspark.sql.functions import isnan, when, count, col

df.select([count(when(isnan(c),c)).alias(c) for c in df.columns]).toPandas().head()

"""#Correlation"""

numeric_features = [t[0] for t in df.dtypes if t[1] !='string']
numeric_features_df = df.select(numeric_features)
numeric_features_df.toPandas().head()

col_names = numeric_features_df.columns
features = numeric_features_df.rdd.map(lambda row: row[0:])

from pyspark.mllib.stat import Statistics
import pandas as pd

corr_mat = Statistics.corr(features, method="pearson")
corr_df = pd.DataFrame(corr_mat)
corr_df.index = col_names
corr_df.columns = col_names
round(corr_df, 2)

from pyspark.sql.functions import corr

#sns.heatmap(corr_df);
sns.heatmap(corr_df, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.savefig("my_plot.png", dpi=300)  # "my_plot.png" is the file name

target_column = "POSTURE"

feature_columns = [col for col in df.columns if col != target_column]

feature_columns

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import ChiSquareTest

# Inisialisasi sesi Spark
spark = SparkSession.builder.appName("PValueCalculation").getOrCreate()

# Membaca dataset (gantilah path dengan path yang sesuai)
data_path = "PosturedanPersonality_1.csv"
df = spark.read.csv(data_path, header=True, inferSchema=True)

# Menampilkan beberapa data
df.show()

# Menggunakan kolom target (variabel y) yang sesuai
target_column = "POSTURE"  # Ganti dengan nama kolom target yang sesuai

# Mengumpulkan nama kolom fitur (tidak termasuk kolom target)
feature_columns = [col for col in df.columns if col != target_column]

# Membuat assembler vektor fitur
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
df = assembler.transform(df).select("features", target_column)

# Menghitung p-value untuk setiap fitur dengan ChiSquareTest
test_results = ChiSquareTest.test(df, "features", target_column)

# Menampilkan hasil
p_values = test_results.select("pValues").collect()[0][0]

for i, feature in enumerate(feature_columns):
    print(f"Feature: {feature}, P-Value: {p_values[i]}")

# Stop sesi Spark
spark.stop()